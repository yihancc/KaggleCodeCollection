{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-08T14:38:33.498766Z","iopub.status.busy":"2023-11-08T14:38:33.498373Z","iopub.status.idle":"2023-11-08T14:38:33.506165Z","shell.execute_reply":"2023-11-08T14:38:33.505115Z","shell.execute_reply.started":"2023-11-08T14:38:33.498731Z"},"trusted":true},"outputs":[],"source":["## This note book comes from https://www.kaggle.com/code/isbhargav/guide-to-pytorch-learning-rate-scheduling"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T14:38:38.384867Z","iopub.status.busy":"2023-11-08T14:38:38.384313Z","iopub.status.idle":"2023-11-08T14:38:39.899636Z","shell.execute_reply":"2023-11-08T14:38:39.898646Z","shell.execute_reply.started":"2023-11-08T14:38:38.384832Z"},"trusted":true},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["\n","# 1. LAMBDA LR\n","\n","Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.\n","\n","$$\n","l r_{\\text {epoch}} = l r_{\\text {initial}} * Lambda(epoch)\n","$$\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-08T14:39:05.990148Z","iopub.status.busy":"2023-11-08T14:39:05.989621Z","iopub.status.idle":"2023-11-08T14:39:06.180056Z","shell.execute_reply":"2023-11-08T14:39:06.179102Z","shell.execute_reply.started":"2023-11-08T14:39:05.990090Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7930b3960c90>]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","lambda1 = lambda epoch: 0.65 ** epoch\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n","\n","\n","lrs = []\n","\n","for i in range(10):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \", round(0.65 ** i,3),\" , Learning Rate = \",round(optimizer.param_groups[0][\"lr\"],3))\n","    scheduler.step()\n","\n","plt.plot(range(10),lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. MultiplicativeLR\n","\n","Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.\n","\n","$$\n","l r_{\\text {epoch}} = l r_{\\text {epoch - 1}} * Lambda(epoch)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","lmbda = lambda epoch: 0.65 ** epoch\n","scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","lrs = []\n","\n","for i in range(10):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",0.95,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(range(10),lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# 3. StepLR\n","\n","Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n","\n","$$\n","l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll}\n","Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text {epoch % step_size}}=0 \\\\\n","l r_{\\text {epoch - 1}}, & \\text { otherwise }\n","\\end{array}\\right.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n","lrs = []\n","\n","for i in range(10):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",0.1 if i!=0 and i%2!=0 else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(range(10),lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. MultiStepLR\n","\n","Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.\n","\n","$$\n","l r_{\\text {epoch}}=\\left\\{\\begin{array}{ll}\n","Gamma * l r_{\\text {epoch - 1}}, & \\text { if } {\\text{ epoch in [milestones]}} \\\\\n","l r_{\\text {epoch - 1}}, & \\text { otherwise }\n","\\end{array}\\right.\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[6,8,9], gamma=0.1)\n","lrs = []\n","\n","for i in range(10):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",0.1 if i in [6,8,9] else 1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(range(10),lrs)"]},{"cell_type":"markdown","metadata":{},"source":["# 5. ExponentialLR\n","\n","Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.\n","\n","$$\n","l r_{\\text {epoch}}= Gamma * l r_{\\text {epoch - 1}}\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n","lrs = []\n","\n","\n","for i in range(10):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",0.1,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 6. CosineAnnealingLR\n","\n","Set the learning rate of each parameter group using a cosine annealing schedule.\n","When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:\n","\n","$$\n","\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{c u r}}{T_{\\max }} \\pi\\right)\\right)\n","$$\n","\n","It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.https://arxiv.org/abs/1608.03983"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. CyclicLR - triangular\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular\")\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. CyclicLR - triangular2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"triangular2\")\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. CyclicLR - exp_range\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=100)\n","scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1,step_size_up=5,mode=\"exp_range\",gamma=0.85)\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 8.OneCycleLR - cos\n","\n","Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates.\n","\n","The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training.\n","\n","This scheduler is not chainable."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10)\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 8.OneCycleLR - linear"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=10,anneal_strategy='linear')\n","lrs = []\n","\n","\n","for i in range(100):\n","    optimizer.step()\n","    lrs.append(optimizer.param_groups[0][\"lr\"])\n","#     print(\"Factor = \",i,\" , Learning Rate = \",optimizer.param_groups[0][\"lr\"])\n","    scheduler.step()\n","\n","plt.plot(lrs)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 9.CosineAnnealingWarmRestarts\n","Set the learning rate of each parameter group using a cosine annealing schedule, and restarts after Ti epochs.\n","\n","\n","$$\n","\\eta_{t}=\\eta_{\\min }+\\frac{1}{2}\\left(\\eta_{\\max }-\\eta_{\\min }\\right)\\left(1+\\cos \\left(\\frac{T_{\\operatorname{cur}}}{T_{i}} \\pi\\right)\\right)\n","$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=0.001, last_epoch=-1)\n","\n","\n","lrs = []\n","\n","for i in range(100):\n","    lr_sched.step()\n","    lrs.append(\n","        optimizer.param_groups[0][\"lr\"]\n","    )\n","\n","plt.plot(lrs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","\n","model = torch.nn.Linear(2, 1)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n","lr_sched = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.01, last_epoch=-1)\n","\n","\n","lrs = []\n","\n","for i in range(300):\n","    lr_sched.step()\n","    lrs.append(\n","        optimizer.param_groups[0][\"lr\"]\n","    )\n","\n","plt.plot(lrs)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
